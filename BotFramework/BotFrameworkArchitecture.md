<link href="css/custom.css" rel="stylesheet"></link> 

# Bot Framework Architecture

## Overview

In the last few years, evolution of devices, platforms and applications have improved user experience and, as a result, expectations. Voice activated digital assistants like Siri and Cortana allow users to interact with services and information in new different ways. 

Perhaps it is time to replace the old web forms with more natural, conversational interaction with users.

In 2016 conference Microsoft introduced its [Bot Framework](https://dev.botframework.com/), which provides a platform for developers to build **intelligent conversation agents**, aka **bots**, and connect them via growing list of **channels** such as Skype, Facebook Messenger,Slack, Telegram and an embed web chat widget.
If combined with the services and APIs offered in [Microsoft Cognitive Services](https://azure.microsoft.com/en-us/services/cognitive-services/), such as [Language Understanding Intelligent Service (LUIS)](https://azure.microsoft.com/en-us/services/cognitive-services/language-understanding-intelligent-service/), bots have the potential to provide rich and useful interactions with users.

## Bot Framework Architecture

The following picture provides a bird's eye view of the Microsoft Bot Framework architecture.

![Bot Architecture](Media/bot_architecture_2.PNG)

Let's describe the various components.

### Your Bot

Typically, a bot is implemented as a **standard Web service which exposes a REST API**. You can implement it with any web technology stack you prefer. 
Microsoft provides a [Bot Framework SDK](https://docs.microsoft.com/en-us/azure/bot-service/?view=azure-bot-service-4.0) for .NET (C#} and JavaScript (Node.js).
Both SDKs are free, open source and hosted on GitHub at this location: [botframework-sdk](https://github.com/microsoft/botframework-sdk)
The Bot Framework SDK gives you two primary tools for building your bot:

1. **Adapter**
The Adapter is a necessary component of every bot. After the web service endpoint (under **/api/messages**) receives a message from the user (or generally speaking, an **Activity**), it is forwarded to the **Adapter**. The Adapter unwraps it, performs authentication, maps it to the user, etc. Then it creates an **TurnContext** object, which the actual bot code can process in the current **Turn**.

1. **Turn & TurnContext**
The Bot Framework v4 represents interactions between users and the bot as Turns. **Each Activity a user performs generates a new Turn**. For example, a message from the user to the bot will imply a new Turn, but there are numerous activities that also imply a new turn.
For each Turn, the bot receives a TurnContext object (generated by the Adapter). The TurnContext contains information about the current conversation, the activity that triggered the turn, the user state and further data points.

1. **Activities**
Activities are the events that our bot receives from its users. Probably the two most prominent activities are `conversationUpdate` and `message`. While message is self-explanatory (a message sent from the user to the bot), the `conversationUpdate` is triggered when a user or the bot join a conversation. Other Activities include `contactRelationUpdate` (when user adds or removes the bot to/from the contact list) or `typing` (triggered when the user is typing). A full list of all supported Activities can be found at [Entities and activity types](https://docs.microsoft.com/en-us/azure/bot-service/bot-service-activities-entities?view=azure-bot-service-4.0&tabs=js#activity-types).


1. **Middleware**
The Adapter produces the TurnContext object by passing the initial request through the Bot Framework’s Middleware. The Middleware is a pipeline that for example restores the state of the conversation, and potentially performs language understanding or translation. This Middleware pipeline can be extended with additional processing steps and is executed on every incoming message.

1. **State and Persistency**
The Middleware component automatically restores the conversation state (e.g., the dialog in which the conversation with the user is in) and also restores any custom user state. In contrast to Bot Framework v3, in v4 you are responsible for manually updating both states. Both Azure Blob and CosmosDB are supported targets for persisting state.

1. **Conversation Flow**
This is probably the area where the most changes happened in v4.
First of all, **Dialogs** are not a “must” in v4. For example, bots and assistants that perform single-shot operations, e.g. “turn off the lights” you probably do not need to use any dialogs, but rather just use regular classes and call their methods.
However, **Dialogs** are necessary for more complex and nested conversations.
A Dialog is composed of one or more **WaterfallSteps**. This allows for a linear conversation flow, as indicated in this example:
    1. Dialog starts
       1. Waterfall Step 1: Bot asks something
          1. User answers
       1. Waterfall Step 2: Bot processes the response and asks something else
          1. User answers
       1. Waterfall Step 3: Bot processes the response and answers
    1. Dialog ends

    In this example, the Dialog would contain 3 WaterfallStep entries.
    Step 1 and 2 would contain a Prompt. A Prompt is a single-step Dialog that asks the user something. The concept of Prompts is similar its counterpart in v3 and several built-in Prompts are included. However, custom Prompts with custom Validators can be written for better reusability of code.

    How does a Dialog receive data? Similar to v3, either by passing (or having it passed) into the Dialog via the TurnContext object or by accessing the custom user state data.

    Multiple Dialogs and Prompts are grouped together in a **DialogSet**. As a Dialog can not have child-Dialogs any more, DialogSets are the way to group Dialogs and Prompts. 
    In v3, we often used a Root Dialog and routed to the individual sub-dialogs. In v4, we would have a DialogSet as the Root Dialog, containing all our sub-Dialogs.


### Language Understanding Intelligent Service (LUIS)

The Bot Framework SDK facilitates the integration of [LUIS](https://azure.microsoft.com/en-us/services/cognitive-services/language-understanding-intelligent-service/), models for language understanding. LUIS helps a bot to parse messages to understand the user’s intent and any related entities.

#### Training LUIS

LUIS actively learns based on the messages it receives, so it is continuously improving. You can review recognized and unrecognized messages to further train LUIS for your model.
The [LuisDialog](https://docs.botframework.com/en-us/csharp/builder/sdkreference/d8/df9/class_microsoft_1_1_bot_1_1_builder_1_1_dialogs_1_1_luis_dialog.html) class in the Bot Framework SDK makes it simple to wire up a LUIS application to call the appropriate methods on your Dialog based on Intent parsing, and passing any parsed entities along the way.

### Beyond Text

The latest release of the Bot Framework SDK includes support for richer content such as **cards**, **carousels** and **buttons**. Channels that are not capable to display the richer content, and others (like SMS texting) fall back to text.

You can also integrate other services with your bot, such as the [Bing Speech API](https://www.microsoft.com/cognitive-services/en-us/speech-api/) and Bing search APIs like [Web](https://www.microsoft.com/cognitive-services/en-us/bing-web-search-api), [Image](https://www.microsoft.com/cognitive-services/en-us/bing-image-search-api) and [Video](https://www.microsoft.com/cognitive-services/en-us/bing-video-search-api). 
You can also integrate big data analytics and machine learning through technologies such as [Cortana](https://docs.microsoft.com/en-us/azure/bot-service/bot-service-channel-connect-cortana?view=azure-bot-service-4.0), to help build even more intelligent bots.



## Glossary

1. **Activity**. Interaction between the user and the bot. Any interaction generates an **activity** object. Examples of activity are:
    - **conversation update**
    - **message**
1. **Turn**. It consists of the following:
    - The user's incoming activity to the bot.
    - Any activity the bot sends back to the user as an immediate response.
1. **Middleware**. It is a set of components which are executed in a predefined order (pipeline) to operate on an activity.
1. **Adapter**. It is an integrated component of the SDK, is the core of the SDK runtime. An activity is carried as JSON in the HTTP POST body. This JSON is deserialized to create the **activity** object that is then handed to the **adapter** with a call to process activity method. On receiving the activity, the adapter creates a **turn context** and calls the middleware.



## References

| Topic | Description |
| :--- | :--- |
| [Bot Framework SDK](https://www.appliedis.com/the-bot-framework/)| Architectural notes |
| [LUIS](https://azure.microsoft.com/en-us/services/cognitive-services/language-understanding-intelligent-service/)|Language Understanding Intelligent Service|
|[Logical architectures for chatbots](https://www.techinasia.com/talk/primer-chatbots-logical-architecture)|A primer on logical architectures for chatbots|
|[Typical architecture of an AI chatbot?](https://www.quora.com/What-is-the-typical-architecture-of-an-AI-chatbot)|What is the typical architecture of an AI chatbot?|
|[Microsoft Bot Framework v4 explained](https://clemenssiebler.com/microsoft-bot-framework-v4-explained-javascript/)|Microsoft Bot Framework v4 explained (JavaScript)|
|[Overview of the architecture of chatbots](https://bigdata-madesimple.com/how-do-chatbots-work-an-overview-of-the-architecture-of-a-chatbot/)|How do chatbots work? An overview of the architecture of chatbots|
|[Guide To Chatbot Tech Stack](https://chatbotsmagazine.com/the-ultimate-guide-to-designing-a-chatbot-tech-stack-333eceb431da)|The Ultimate Guide To Designing A Chatbot Tech Stack|
|[Using Microsoft Bot Framework v3](https://www.udemy.com/using-microsoft-bot-framework-luis-and-cognitive-services/)|Using Microsoft Bot Framework v3, LUIS, Cognitive Services (Udemy) |
|[]()|TBD|
